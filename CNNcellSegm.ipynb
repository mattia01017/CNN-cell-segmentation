{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattia01017/CNN-cell-segmentation/blob/main/CNNcellSegm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110YGH3Sg7z9"
      },
      "source": [
        "## Project: CNN-based segmentation of cells in multi-modal microscopy images\n",
        "\n",
        "Contact: Karl Rohr (Heidelberg University K.Rohr@dkfz-heidelberg.de)\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Cell segmentation is a central task in biomedical image analysis and enables counting the cell number, quantifying single-cell fluorescence intensity, and tracking of cells to analyze cell motion. Challenges are high variation of cell shape and image intensity, strong image noise, low image contrast, and high cell density. Deep learning methods for cell segmentation in microscopy data show promising results.\n",
        "\n",
        "### Goal\n",
        "\n",
        "The goal of this project is to develop a Convolutional Neural Network (CNN) to automatically segment cells in microscopy images. A modified 2D U-Net model (Ronneberger et al., MICCAI 2015, Falk et al., Nature Methods 2019) will be implemented and applied to image data from different imaging modalities comprising fluorescence, differential interference contrast (DIC), and phase-contrast microscopy. Different versions of the network model will be generated by training on data from individual imaging modalities as well as a combination of different imaging modalities. The trained models are applied to the different datasets, and the segmentation accuracy will be quantified by a performance metric. The experimental results are analyzed and a comparison of the different network models will be carried out.\n",
        "\n",
        "### Data and model\n",
        "\n",
        "The [2D U-Net model](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) will be used as a basis. Zero-padding (SAME padding) in the convolution layers should be included to ensure that the output size of the network is the same as the input size. This eliminates the need for cropping and resizing of the images in the original model. Experiments with different network configurations should be performed (e.g., different number of layers). Besides using a Cross-Entropy Loss as in the original model, a Dice Loss or a combination of both losses should be investigated. Also, loss weighting as in the original model should be tested. Existing implementations of the U-Net can be used as inspiration.\n",
        "\n",
        "The network model should be applied to [datasets from the Cell Tracking Challenge](https://celltrackingchallenge.net/2d-datasets/) (Ulman et al., Nature Methods 2017) comprising 2D time-lapse microscopy image data of three different imaging modalities:\n",
        "1. Fluorescence microscopy: HeLa cells stably expressing H2b-GFP ([download](http://data.celltrackingchallenge.net/training-datasets/Fluo-N2DL-HeLa.zip))\n",
        "2. Differential interference contrast (DIC) microscopy: HeLa cells on a flat glass ([download](http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip))\n",
        "3. Phase-contrast microscopy: Glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate ([download](http://data.celltrackingchallenge.net/training-datasets/PhC-C2DH-U373.zip))\n",
        "\n",
        "Each dataset consists of two image sequences from which single images should be used as samples. The images can be found in the folders `01` and `02`. Since the \"gold truth\" provided in the folders `01_GT/SEG` and `02_GT/SEG` is not available for all single images or cells, the \"silver truth\" in `01_ST/SEG` or `02_ST/SEG`can be used. More information on the datasets can be found [here](https://celltrackingchallenge.net/). One of the two image sequences should be used for training and the other for testing. The labels are instance labels (with a different ID for each cell), which need to be converted to semantic labels (foreground-background).\n",
        "\n",
        "Different versions of the network model should be generated by training on data from individual imaging modalities as well as a combination of different imaging modalities. A comparison of the different network models should be performed. The Jaccard Index (Intersection over Union) can be used as segmentation performance metric. A strategy needs to be found to deal with large image sizes (e.g., using image resizing or image tiling as in the original model). To improve the segmentation result, different preprocessing and data augmentation strategies can be studied. Information on deep learning for image segmentation and the U-Net is provided in the lecture (Karl Rohr).\n",
        "\n",
        "\n",
        "### References\n",
        "Ronneberger 0, Fischer P, Brox T. (2015) [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28), MICCAI 2015, 234–241\n",
        "\n",
        "Falk T, Brox T, Ronneberger O et al. (2019) [U-Net: deep learning for cell counting, detection, and morphometry](https://www.nature.com/articles/s41592-018-0261-2), Nature Methods 16, 67-70\n",
        "\n",
        "Ulman V, Maška M, Harder N, Rohr K, Kozubek M, Ortiz-de-Solorzano C, et al. (2017) [An objective comparison of cell-tracking algorithms](https://www.nature.com/articles/nmeth.4473), Nature Methods 14, 1141–1152\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Bq0lGBg70B"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWEE56K_g70C"
      },
      "outputs": [],
      "source": [
        "!curl https://data.celltrackingchallenge.net/training-datasets/Fluo-N2DL-HeLa.zip -o Fluo-N2DL-HeLa.zip\n",
        "!curl https://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip -o DIC-C2DH-HeLa.zip\n",
        "!curl https://data.celltrackingchallenge.net/training-datasets/PhC-C2DH-U373.zip -o PhC-C2DH-U373.zip\n",
        "!unzip Fluo-N2DL-HeLa.zip > /dev/null\n",
        "!unzip DIC-C2DH-HeLa.zip > /dev/null\n",
        "!unzip PhC-C2DH-U373.zip > /dev/null\n",
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset exploration"
      ],
      "metadata": {
        "id": "sBGhTT0aF5cp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-emzEeuF7x9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}